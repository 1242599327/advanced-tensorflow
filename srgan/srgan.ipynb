{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPER-RESOLUTION GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PACKAGES LOADED.\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "imageio.plugins.ffmpeg.download()\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import scipy.misc \n",
    "from scipy.misc import imread, imresize\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline  \n",
    "print (\"PACKAGES LOADED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RESET EXISTING FLAG \n",
    "tf.app.flags.FLAGS = tf.python.platform.flags._FlagValues()\n",
    "tf.app.flags._global_parser = argparse.ArgumentParser()\n",
    "# SET FLAGS\n",
    "tf.app.flags.DEFINE_integer('batch_size', 16,\n",
    "                            \"Number of samples per batch.\")\n",
    "tf.app.flags.DEFINE_string('checkpoint_dir', 'checkpoint',\n",
    "                           \"Output folder where checkpoints are dumped.\")\n",
    "tf.app.flags.DEFINE_integer('checkpoint_period', 10000,\n",
    "                            \"Number of batches in between checkpoints\")\n",
    "tf.app.flags.DEFINE_string('dataset', 'dataset',\n",
    "                           \"Path to the dataset directory.\")\n",
    "tf.app.flags.DEFINE_float('epsilon', 1e-8,\n",
    "                          \"Fuzz term to avoid numerical instability\")\n",
    "tf.app.flags.DEFINE_string('run', 'demo',\n",
    "                            \"Which operation to run. [demo|train]\")\n",
    "tf.app.flags.DEFINE_float('gene_l1_factor', .90,\n",
    "                          \"Multiplier for generator L1 loss term\")\n",
    "tf.app.flags.DEFINE_float('learning_beta1', 0.5,\n",
    "                          \"Beta1 parameter used for AdamOptimizer\")\n",
    "tf.app.flags.DEFINE_float('learning_rate_start', 0.00020,\n",
    "                          \"Starting learning rate used for AdamOptimizer\")\n",
    "tf.app.flags.DEFINE_integer('learning_rate_half_life', 5000,\n",
    "                            \"Number of batches until learning rate is halved\")\n",
    "tf.app.flags.DEFINE_bool('log_device_placement', False,\n",
    "                         \"Log the device where variables are placed.\")\n",
    "tf.app.flags.DEFINE_integer('sample_size', 64,\n",
    "                            \"Image sample size in pixels. Range [64,128]\")\n",
    "tf.app.flags.DEFINE_integer('summary_period', 500,\n",
    "                            \"Number of batches between summary data dumps\")\n",
    "tf.app.flags.DEFINE_integer('random_seed', 0,\n",
    "                            \"Seed used to initialize rng.\")\n",
    "tf.app.flags.DEFINE_integer('test_vectors', 16,\n",
    "                            \"\"\"Number of features to use for testing\"\"\")\n",
    "tf.app.flags.DEFINE_string('train_dir', 'train',\n",
    "                           \"Output folder where training logs are dumped.\")\n",
    "tf.app.flags.DEFINE_integer('train_time', 20,\n",
    "                            \"Time in minutes to train the model\")\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_tensorflow():\n",
    "    # Create session\n",
    "    config = tf.ConfigProto(log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=config)\n",
    "    # Initialize rng with a deterministic seed\n",
    "    with sess.graph.as_default():\n",
    "        tf.set_random_seed(FLAGS.random_seed)\n",
    "    random.seed(FLAGS.random_seed)\n",
    "    np.random.seed(FLAGS.random_seed)\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)\n",
    "    return sess, summary_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETUP TF SESSION AND SUMMARY\n"
     ]
    }
   ],
   "source": [
    "sess, summary_writer = setup_tensorflow()\n",
    "print (\"SETUP TF SESSION AND SUMMARY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_dirs(delete_train_dir=False):\n",
    "    # CREATE CHECKPOINT DIRECTORIES\n",
    "    if not tf.gfile.Exists(FLAGS.checkpoint_dir):\n",
    "        tf.gfile.MakeDirs(FLAGS.checkpoint_dir)\n",
    "        print (\"CREATE CHECKPOINT FOLDER[%s]\" % (FLAGS.checkpoint_dir))\n",
    "    else:\n",
    "        print (\"CHECKPOINT FOLDER[%s] ALREADY EXISTS\" % (FLAGS.checkpoint_dir))\n",
    "    # CLEANUP TRAIN DIR\n",
    "    if delete_train_dir:\n",
    "        if tf.gfile.Exists(FLAGS.train_dir):\n",
    "            tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "            print (\"DELETE EVERY FILES IN TRAIN FOLDER[%s]\" % (FLAGS.train_dir))\n",
    "        tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "        print (\"CREATE TRAIN FOLDER[%s]\" % (FLAGS.train_dir))\n",
    "        \n",
    "    # Return names of training files\n",
    "    if not tf.gfile.Exists(FLAGS.dataset) or \\\n",
    "        not tf.gfile.IsDirectory(FLAGS.dataset):\n",
    "        print (\"DATASET FOLDER[%s] DOES NOT EXIST\" % (FLAGS.dataset))\n",
    "        return\n",
    "    else:\n",
    "        print (\"DATASET FOLDER[%s] EXISTS\" % (FLAGS.dataset))\n",
    "    # LOAD FILES IN THE DATASET FOLDER\n",
    "    filenames = tf.gfile.ListDirectory(FLAGS.dataset)\n",
    "    filenames = sorted(filenames)\n",
    "    random.shuffle(filenames)\n",
    "    filenames = [os.path.join(FLAGS.dataset, f) for f in filenames]\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECKPOINT FOLDER[checkpoint] ALREADY EXISTS\n",
      "DELETE EVERY FILES IN TRAIN FOLDER[train]\n",
      "CREATE TRAIN FOLDER[train]\n",
      "DATASET FOLDER[dataset] EXISTS\n",
      "[202599] IMAGES LOADED.\n"
     ]
    }
   ],
   "source": [
    "all_filenames = prepare_dirs(delete_train_dir=True)\n",
    "print (\"[%d] IMAGES LOADED.\" % (len(all_filenames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SEPARATE TRAINING AND TEST SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202583] TRAIN IMAGES\n",
      "[16] TEST IMAGES\n"
     ]
    }
   ],
   "source": [
    "train_filenames = all_filenames[:-FLAGS.test_vectors]\n",
    "test_filenames  = all_filenames[-FLAGS.test_vectors:]\n",
    "print (\"[%d] TRAIN IMAGES\" % (len(train_filenames)))\n",
    "print (\"[%d] TEST IMAGES\" % (len(test_filenames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_inputs(sess, filenames, image_size=None, capacity_factor=3):\n",
    "    if image_size is None:\n",
    "        image_size = FLAGS.sample_size\n",
    "    # READ JPEG IMAGES\n",
    "    reader = tf.WholeFileReader()\n",
    "    filename_queue = tf.train.string_input_producer(filenames)\n",
    "    key, value = reader.read(filename_queue)\n",
    "    channels = 3\n",
    "    image = tf.image.decode_jpeg(value, channels=channels, name=\"dataset_image\")\n",
    "    image.set_shape([None, None, channels])\n",
    "    # RANDOM AUGMENTATIONS\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_saturation(image, .95, 1.05)\n",
    "    image = tf.image.random_brightness(image, .05)\n",
    "    image = tf.image.random_contrast(image, .95, 1.05)\n",
    "    # RANDOM CROP\n",
    "    wiggle = 8\n",
    "    off_x, off_y = 25-wiggle, 60-wiggle\n",
    "    crop_size = 128\n",
    "    crop_size_plus = crop_size + 2*wiggle\n",
    "    image = tf.image.crop_to_bounding_box(image, off_y, off_x, crop_size_plus, crop_size_plus)\n",
    "    image = tf.random_crop(image, [crop_size, crop_size, 3])\n",
    "    image = tf.reshape(image, [1, crop_size, crop_size, 3])\n",
    "    image = tf.cast(image, tf.float32)/255.0\n",
    "    if crop_size != image_size:\n",
    "        image = tf.image.resize_area(image, [image_size, image_size])\n",
    "\n",
    "    # THE INPUT IS SIMPLY A K-DOWNSCALED VERSION\n",
    "    K = 4\n",
    "    downsampled = tf.image.resize_area(image, [image_size//K, image_size//K])\n",
    "    feature = tf.reshape(downsampled, [image_size//K, image_size//K, 3])\n",
    "    label   = tf.reshape(image,       [image_size,   image_size,     3])\n",
    "    # USE ASYNCHRONOUS QUEUES \n",
    "    features, labels = tf.train.batch([feature, label],\n",
    "                                      batch_size=FLAGS.batch_size,\n",
    "                                      num_threads=4,\n",
    "                                      capacity = capacity_factor*FLAGS.batch_size,\n",
    "                                      name='labels_and_features')\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH SIZE IS [16]\n",
      "train_features is [Tensor(\"labels_and_features:0\", shape=(16, 16, 16, 3), dtype=float32)]\n",
      "train_labels is [Tensor(\"labels_and_features:1\", shape=(16, 64, 64, 3), dtype=float32)]\n",
      "test_features is [Tensor(\"labels_and_features_1:0\", shape=(16, 16, 16, 3), dtype=float32)]\n",
      "test_labels is [Tensor(\"labels_and_features_1:1\", shape=(16, 64, 64, 3), dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = setup_inputs(sess, train_filenames)\n",
    "test_features,  test_labels  = setup_inputs(sess, test_filenames)\n",
    "print (\"BATCH SIZE IS [%d]\"     % (FLAGS.batch_size))\n",
    "print (\"train_features is [%s]\" % (train_features))\n",
    "print (\"train_labels is [%s]\"   % (train_labels)) \n",
    "print (\"test_features is [%s]\"  % (test_features)) \n",
    "print (\"test_labels is [%s]\"    % (test_labels)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDING NOISE TO TRAIN FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_level = .03\n",
    "noisy_train_features = train_features + \\\n",
    "    tf.random_normal(train_features.get_shape(), stddev=noise_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:    \n",
    "    def __init__(self, name, features):\n",
    "        self.name = name\n",
    "        self.outputs = [features]\n",
    "    def _get_layer_str(self, layer=None):\n",
    "        if layer is None:\n",
    "            layer = self.get_num_layers()\n",
    "        return '%s_L%03d' % (self.name, layer+1)\n",
    "    def _get_num_inputs(self):\n",
    "        return int(self.get_output().get_shape()[-1])\n",
    "    def _glorot_initializer(self, prev_units, num_units, stddev_factor=1.0):\n",
    "        \"\"\"Initialization in the style of Glorot 2010.\n",
    "        stddev_factor should be 1.0 for linear activations, and 2.0 for ReLUs\"\"\"\n",
    "        stddev  = np.sqrt(stddev_factor / np.sqrt(prev_units*num_units))\n",
    "        return tf.truncated_normal([prev_units, num_units],\n",
    "                                    mean=0.0, stddev=stddev)\n",
    "    def _glorot_initializer_conv2d(self, prev_units, num_units, mapsize, stddev_factor=1.0):\n",
    "        \"\"\"Initialization in the style of Glorot 2010.\n",
    "        stddev_factor should be 1.0 for linear activations, and 2.0 for ReLUs\"\"\"\n",
    "        stddev  = np.sqrt(stddev_factor / (np.sqrt(prev_units*num_units)*mapsize*mapsize))\n",
    "        return tf.truncated_normal([mapsize, mapsize, prev_units, num_units],\n",
    "                                    mean=0.0, stddev=stddev)\n",
    "    def get_num_layers(self):\n",
    "        return len(self.outputs)\n",
    "    def add_batch_norm(self, scale=False):\n",
    "        \"\"\"Adds a batch normalization layer to this model.\n",
    "        See ArXiv 1502.03167v3 for details.\"\"\"\n",
    "        # TBD: This appears to be very flaky, often raising InvalidArgumentError internally\n",
    "        with tf.variable_scope(self._get_layer_str()):\n",
    "            out = tf.contrib.layers.batch_norm(self.get_output(), scale=scale)\n",
    "        self.outputs.append(out)\n",
    "        return self\n",
    "    def add_flatten(self):\n",
    "        \"\"\"Transforms the output of this network to a 1D tensor\"\"\"\n",
    "        with tf.variable_scope(self._get_layer_str()):\n",
    "            batch_size = int(self.get_output().get_shape()[0])\n",
    "            out = tf.reshape(self.get_output(), [batch_size, -1])\n",
    "        self.outputs.append(out)\n",
    "        return self\n",
    "    def add_dense(self, num_units, stddev_factor=1.0):\n",
    "        \"\"\"Adds a dense linear layer to this model.\n",
    "        Uses Glorot 2010 initialization assuming linear activation.\"\"\"\n",
    "        assert len(self.get_output().get_shape()) == 2, \"Previous layer must be 2-dimensional (batch, channels)\"\n",
    "        with tf.variable_scope(self._get_layer_str()):\n",
    "            prev_units = self._get_num_inputs()\n",
    "            # Weight term\n",
    "            initw   = self._glorot_initializer(prev_units, num_units,\n",
    "                                               stddev_factor=stddev_factor)\n",
    "            weight  = tf.get_variable('weight', initializer=initw)\n",
    "            # Bias term\n",
    "            initb   = tf.constant(0.0, shape=[num_units])\n",
    "            bias    = tf.get_variable('bias', initializer=initb)\n",
    "            # Output of this layer\n",
    "            out     = tf.matmul(self.get_output(), weight) + bias\n",
    "        self.outputs.append(out)\n",
    "        return self\n",
    "    def add_sigmoid(self):\n",
    "        \"\"\"Adds a sigmoid (0,1) activation function layer to this model.\"\"\"\n",
    "        with tf.variable_scope(self._get_layer_str()):\n",
    "            prev_units = self._get_num_inputs()\n",
    "            out = tf.nn.sigmoid(self.get_output())\n",
    "        self.outputs.append(out)\n",
    "        return self\n",
    "    def add_softmax(self):\n",
    "        \"\"\"Adds a softmax operation to this model\"\"\"\n",
    "        with tf.variable_scope(self._get_layer_str()):\n",
    "            this_input = tf.square(self.get_output())\n",
    "            reduction_indices = list(range(1, len(this_input.get_shape())))\n",
    "            acc = tf.reduce_sum(this_input, reduction_indices=reduction_indices, keep_dims=True)\n",
    "            out = this_input / (acc+FLAGS.epsilon)\n",
    "            #out = tf.verify_tensor_all_finite(out, \"add_softmax failed; is sum equal to zero?\")\n",
    "        self.outputs.append(out)\n",
    "        return self\n",
    "    def add_relu(self):\n",
    "        \"\"\"Adds a ReLU activation function to this model\"\"\"\n",
    "        with tf.variable_scope(self._get_layer_str()):\n",
    "            out = tf.nn.relu(self.get_output())\n",
    "        self.outputs.append(out)\n",
    "        return self        \n",
    "    def add_elu(self):\n",
    "        \"\"\"Adds a ELU activation function to this model\"\"\"\n",
    "        with tf.variable_scope(self._get_layer_str()):\n",
    "            out = tf.nn.elu(self.get_output())\n",
    "        self.outputs.append(out)\n",
    "        return self\n",
    "    def add_lrelu(self, leak=.2):\n",
    "        \"\"\"Adds a leaky ReLU (LReLU) activation function to this model\"\"\"\n",
    "        with tf.variable_scope(self._get_layer_str()):\n",
    "            t1  = .5 * (1 + leak)\n",
    "            t2  = .5 * (1 - leak)\n",
    "            out = t1 * self.get_output() + \\\n",
    "                  t2 * tf.abs(self.get_output())\n",
    "        self.outputs.append(out)\n",
    "        return self\n",
    "    def add_conv2d(self, num_units, mapsize=1, stride=1, stddev_factor=1.0):\n",
    "        \"\"\"Adds a 2D convolutional layer.\"\"\"\n",
    "        assert len(self.get_output().get_shape()) == 4 and \"Previous layer must be 4-dimensional (batch, width, height, channels)\"\n",
    "        with tf.variable_scope(self._get_layer_str()):\n",
    "            prev_units = self._get_num_inputs()\n",
    "            # Weight term and convolution\n",
    "            initw  = self._glorot_initializer_conv2d(prev_units, num_units,\n",
    "                                                     mapsize,\n",
    "                                                     stddev_factor=stddev_factor)\n",
    "            weight = tf.get_variable('weight', initializer=initw)\n",
    "            out    = tf.nn.conv2d(self.get_output(), weight,\n",
    "                                  strides=[1, stride, stride, 1],\n",
    "                                  padding='SAME')\n",
    "            # Bias term\n",
    "            initb  = tf.constant(0.0, shape=[num_units])\n",
    "            bias   = tf.get_variable('bias', initializer=initb)\n",
    "            out    = tf.nn.bias_add(out, bias)\n",
    "        self.outputs.append(out)\n",
    "        return self\n",
    "    def add_conv2d_transpose(self, num_units, mapsize=1, stride=1, stddev_factor=1.0):\n",
    "        \"\"\"Adds a transposed 2D convolutional layer\"\"\"\n",
    "        assert len(self.get_output().get_shape()) == 4 and \"Previous layer must be 4-dimensional (batch, width, height, channels)\"\n",
    "        with tf.variable_scope(self._get_layer_str()):\n",
    "            prev_units = self._get_num_inputs()\n",
    "            # Weight term and convolution\n",
    "            initw  = self._glorot_initializer_conv2d(prev_units, num_units,\n",
    "                                                     mapsize,\n",
    "                                                     stddev_factor=stddev_factor)\n",
    "            weight = tf.get_variable('weight', initializer=initw)\n",
    "            weight = tf.transpose(weight, perm=[0, 1, 3, 2])\n",
    "            prev_output = self.get_output()\n",
    "            output_shape = [FLAGS.batch_size,\n",
    "                            int(prev_output.get_shape()[1]) * stride,\n",
    "                            int(prev_output.get_shape()[2]) * stride,\n",
    "                            num_units]\n",
    "            out    = tf.nn.conv2d_transpose(self.get_output(), weight,\n",
    "                                            output_shape=output_shape,\n",
    "                                            strides=[1, stride, stride, 1],\n",
    "                                            padding='SAME')\n",
    "            # Bias term\n",
    "            initb  = tf.constant(0.0, shape=[num_units])\n",
    "            bias   = tf.get_variable('bias', initializer=initb)\n",
    "            out    = tf.nn.bias_add(out, bias)\n",
    "        self.outputs.append(out)\n",
    "        return self\n",
    "    def add_residual_block(self, num_units, mapsize=3, num_layers=2, stddev_factor=1e-3):\n",
    "        \"\"\"Adds a residual block as per Arxiv 1512.03385, Figure 3\"\"\"\n",
    "        assert len(self.get_output().get_shape()) == 4 and \"Previous layer must be 4-dimensional (batch, width, height, channels)\"\n",
    "        # Add projection in series if needed prior to shortcut\n",
    "        if num_units != int(self.get_output().get_shape()[3]):\n",
    "            self.add_conv2d(num_units, mapsize=1, stride=1, stddev_factor=1.)\n",
    "        bypass = self.get_output()\n",
    "        # Residual block\n",
    "        for _ in range(num_layers):\n",
    "            self.add_batch_norm()\n",
    "            self.add_relu()\n",
    "            self.add_conv2d(num_units, mapsize=mapsize, stride=1, stddev_factor=stddev_factor)\n",
    "        self.add_sum(bypass)\n",
    "        return self\n",
    "    def add_bottleneck_residual_block(self, num_units, mapsize=3, stride=1, transpose=False):\n",
    "        \"\"\"Adds a bottleneck residual block as per Arxiv 1512.03385, Figure 3\"\"\"\n",
    "        assert len(self.get_output().get_shape()) == 4 and \"Previous layer must be 4-dimensional (batch, width, height, channels)\"\n",
    "        # Add projection in series if needed prior to shortcut\n",
    "        if num_units != int(self.get_output().get_shape()[3]) or stride != 1:\n",
    "            ms = 1 if stride == 1 else mapsize\n",
    "            #bypass.add_batch_norm() # TBD: Needed?\n",
    "            if transpose:\n",
    "                self.add_conv2d_transpose(num_units, mapsize=ms, stride=stride, stddev_factor=1.)\n",
    "            else:\n",
    "                self.add_conv2d(num_units, mapsize=ms, stride=stride, stddev_factor=1.)\n",
    "        bypass = self.get_output()\n",
    "        # Bottleneck residual block\n",
    "        self.add_batch_norm()\n",
    "        self.add_relu()\n",
    "        self.add_conv2d(num_units//4, mapsize=1,       stride=1,      stddev_factor=2.)\n",
    "        self.add_batch_norm()\n",
    "        self.add_relu()\n",
    "        if transpose:\n",
    "            self.add_conv2d_transpose(num_units//4,\n",
    "                                      mapsize=mapsize,\n",
    "                                      stride=1,\n",
    "                                      stddev_factor=2.)\n",
    "        else:\n",
    "            self.add_conv2d(num_units//4,\n",
    "                            mapsize=mapsize,\n",
    "                            stride=1,\n",
    "                            stddev_factor=2.)\n",
    "        self.add_batch_norm()\n",
    "        self.add_relu()\n",
    "        self.add_conv2d(num_units,    mapsize=1,       stride=1,      stddev_factor=2.)\n",
    "        self.add_sum(bypass)\n",
    "        return self\n",
    "    def add_sum(self, term):\n",
    "        \"\"\"Adds a layer that sums the top layer with the given term\"\"\"\n",
    "        with tf.variable_scope(self._get_layer_str()):\n",
    "            prev_shape = self.get_output().get_shape()\n",
    "            term_shape = term.get_shape()\n",
    "            #print(\"%s %s\" % (prev_shape, term_shape))\n",
    "            assert prev_shape == term_shape and \"Can't sum terms with a different size\"\n",
    "            out = tf.add(self.get_output(), term)\n",
    "        self.outputs.append(out)\n",
    "        return self\n",
    "    def add_mean(self):\n",
    "        \"\"\"Adds a layer that averages the inputs from the previous layer\"\"\"\n",
    "        with tf.variable_scope(self._get_layer_str()):\n",
    "            prev_shape = self.get_output().get_shape()\n",
    "            reduction_indices = list(range(len(prev_shape)))\n",
    "            assert len(reduction_indices) > 2 and \"Can't average a (batch, activation) tensor\"\n",
    "            reduction_indices = reduction_indices[1:-1]\n",
    "            out = tf.reduce_mean(self.get_output(), reduction_indices=reduction_indices)\n",
    "        self.outputs.append(out)\n",
    "        return self\n",
    "    def add_upscale(self):\n",
    "        \"\"\"Adds a layer that upscales the output by 2x through nearest neighbor interpolation\"\"\"\n",
    "        prev_shape = self.get_output().get_shape()\n",
    "        size = [2 * int(s) for s in prev_shape[1:3]]\n",
    "        out  = tf.image.resize_nearest_neighbor(self.get_output(), size)\n",
    "        self.outputs.append(out)\n",
    "        return self        \n",
    "    def get_output(self):\n",
    "        \"\"\"Returns the output from the topmost layer of the network\"\"\"\n",
    "        return self.outputs[-1]\n",
    "    def get_variable(self, layer, name):\n",
    "        \"\"\"Returns a variable given its layer and name.\n",
    "        The variable must already exist.\"\"\"\n",
    "        scope      = self._get_layer_str(layer)\n",
    "        collection = tf.get_collection(tf.GraphKeys.VARIABLES, scope=scope)\n",
    "        # TBD: Ugly!\n",
    "        for var in collection:\n",
    "            if var.name[:-2] == scope+'/'+name:\n",
    "                return var\n",
    "        return None\n",
    "    def get_all_layer_variables(self, layer):\n",
    "        \"\"\"Returns all variables in the given layer\"\"\"\n",
    "        scope = self._get_layer_str(layer)\n",
    "        return tf.get_collection(tf.GraphKeys.VARIABLES, scope=scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATOR READY\n"
     ]
    }
   ],
   "source": [
    "def _generator_model(sess, features, labels, channels):\n",
    "    # Upside-down all-convolutional resnet\n",
    "    mapsize = 3\n",
    "    res_units  = [256, 128, 96]\n",
    "    old_vars = tf.global_variables()\n",
    "    # See Arxiv 1603.05027\n",
    "    model = Model('GEN', features)\n",
    "    for ru in range(len(res_units)-1):\n",
    "        nunits  = res_units[ru]\n",
    "        for j in range(2):\n",
    "            model.add_residual_block(nunits, mapsize=mapsize)\n",
    "        # Spatial upscale (see http://distill.pub/2016/deconv-checkerboard/)\n",
    "        # and transposed convolution\n",
    "        model.add_upscale()\n",
    "        model.add_batch_norm()\n",
    "        model.add_relu()\n",
    "        model.add_conv2d_transpose(nunits, mapsize=mapsize, stride=1, stddev_factor=1.)\n",
    "    # Finalization a la \"all convolutional net\"\n",
    "    nunits = res_units[-1]\n",
    "    model.add_conv2d(nunits, mapsize=mapsize, stride=1, stddev_factor=2.)\n",
    "    # Worse: model.add_batch_norm()\n",
    "    model.add_relu()\n",
    "    model.add_conv2d(nunits, mapsize=1, stride=1, stddev_factor=2.)\n",
    "    # Worse: model.add_batch_norm()\n",
    "    model.add_relu()\n",
    "    # Last layer is sigmoid with no batch normalization\n",
    "    model.add_conv2d(channels, mapsize=1, stride=1, stddev_factor=1.)\n",
    "    model.add_sigmoid()\n",
    "    new_vars  = tf.global_variables()\n",
    "    gene_vars = list(set(new_vars) - set(old_vars))\n",
    "    return model.get_output(), gene_vars\n",
    "\n",
    "print (\"GENERATOR READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCRIMINATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISCRIMINATOR READY\n"
     ]
    }
   ],
   "source": [
    "def _discriminator_model(sess, features, disc_input):\n",
    "    # Fully convolutional model\n",
    "    mapsize = 3\n",
    "    layers  = [64, 128, 256, 512]\n",
    "    old_vars = tf.global_variables()\n",
    "    model = Model('DIS', 2*disc_input - 1)\n",
    "    for layer in range(len(layers)):\n",
    "        nunits = layers[layer]\n",
    "        stddev_factor = 2.0\n",
    "        model.add_conv2d(nunits, mapsize=mapsize, stride=2, stddev_factor=stddev_factor)\n",
    "        model.add_batch_norm()\n",
    "        model.add_relu()\n",
    "    # Finalization a la \"all convolutional net\"\n",
    "    model.add_conv2d(nunits, mapsize=mapsize, stride=1, stddev_factor=stddev_factor)\n",
    "    model.add_batch_norm()\n",
    "    model.add_relu()\n",
    "    model.add_conv2d(nunits, mapsize=1, stride=1, stddev_factor=stddev_factor)\n",
    "    model.add_batch_norm()\n",
    "    model.add_relu()\n",
    "    # Linearly map to real/fake and return average score\n",
    "    # (softmax will be applied later)\n",
    "    model.add_conv2d(1, mapsize=1, stride=1, stddev_factor=stddev_factor)\n",
    "    model.add_mean()\n",
    "    new_vars  = tf.global_variables()\n",
    "    disc_vars = list(set(new_vars) - set(old_vars))\n",
    "    return model.get_output(), disc_vars\n",
    "\n",
    "print (\"DISCRIMINATOR READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE SR-GAN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(sess, features, labels):\n",
    "    # Generator\n",
    "    rows      = int(features.get_shape()[1])\n",
    "    cols      = int(features.get_shape()[2])\n",
    "    channels  = int(features.get_shape()[3])\n",
    "    gene_minput = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, rows, cols, channels])\n",
    "    \n",
    "    # TBD: Is there a better way to instance the generator?\n",
    "    with tf.variable_scope('gene') as scope:\n",
    "        gene_output, gene_var_list = \\\n",
    "                    _generator_model(sess, features, labels, channels)\n",
    "        scope.reuse_variables()\n",
    "        gene_moutput, _ = _generator_model(sess, gene_minput, labels, channels)\n",
    "    # Discriminator with real data\n",
    "    disc_real_input = tf.identity(labels, name='disc_real_input')\n",
    "    # TBD: Is there a better way to instance the discriminator?\n",
    "    with tf.variable_scope('disc') as scope:\n",
    "        disc_real_output, disc_var_list = \\\n",
    "                _discriminator_model(sess, features, disc_real_input)\n",
    "        scope.reuse_variables()\n",
    "        disc_fake_output, _ = _discriminator_model(sess, features, gene_output)\n",
    "    return [gene_minput,      gene_moutput,\n",
    "            gene_output,      gene_var_list,\n",
    "            disc_real_output, disc_fake_output, disc_var_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[gene_minput, gene_moutput, gene_output, gene_var_list,\n",
    "    disc_real_output, disc_fake_output, disc_var_list] \\\n",
    "    = create_model(sess, noisy_train_features, train_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATOR LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _downscale(images, K):\n",
    "    \"\"\"Differentiable image downscaling by a factor of K\"\"\"\n",
    "    arr = np.zeros([K, K, 3, 3])\n",
    "    arr[:,:,0,0] = 1.0/(K*K)\n",
    "    arr[:,:,1,1] = 1.0/(K*K)\n",
    "    arr[:,:,2,2] = 1.0/(K*K)\n",
    "    dowscale_weight = tf.constant(arr, dtype=tf.float32)    \n",
    "    downscaled = tf.nn.conv2d(images, dowscale_weight,\n",
    "                              strides=[1, K, K, 1],\n",
    "                              padding='SAME')\n",
    "    return downscaled\n",
    "\n",
    "def create_generator_loss(disc_output, gene_output, features):\n",
    "    # I.e. did we fool the discriminator?\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(disc_output, tf.ones_like(disc_output))\n",
    "    gene_ce_loss  = tf.reduce_mean(cross_entropy, name='gene_ce_loss')\n",
    "    # I.e. does the result look like the feature?\n",
    "    K = int(gene_output.get_shape()[1])//int(features.get_shape()[1])\n",
    "    assert K == 2 or K == 4 or K == 8    \n",
    "    downscaled = _downscale(gene_output, K)\n",
    "    gene_l1_loss  = tf.reduce_mean(tf.abs(downscaled - features), name='gene_l1_loss')\n",
    "    gene_loss     = tf.add((1.0 - FLAGS.gene_l1_factor) * gene_ce_loss,\n",
    "                           FLAGS.gene_l1_factor * gene_l1_loss, name='gene_loss')\n",
    "    return gene_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gene_loss = create_generator_loss(disc_fake_output, gene_output, train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCRIMINATOR LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_discriminator_loss(disc_real_output, disc_fake_output):\n",
    "    # I.e. did we correctly identify the input as real or not?\n",
    "    cross_entropy_real = tf.nn.sigmoid_cross_entropy_with_logits(disc_real_output, tf.ones_like(disc_real_output))\n",
    "    disc_real_loss     = tf.reduce_mean(cross_entropy_real, name='disc_real_loss')\n",
    "    cross_entropy_fake = tf.nn.sigmoid_cross_entropy_with_logits(disc_fake_output, tf.zeros_like(disc_fake_output))\n",
    "    disc_fake_loss     = tf.reduce_mean(cross_entropy_fake, name='disc_fake_loss')\n",
    "    return disc_real_loss, disc_fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_real_loss, disc_fake_loss = \\\n",
    "    create_discriminator_loss(disc_real_output, disc_fake_output)\n",
    "disc_loss = tf.add(disc_real_loss, disc_fake_loss, name='disc_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_optimizers(gene_loss, gene_var_list,\n",
    "                      disc_loss, disc_var_list):    \n",
    "    global_step    = tf.Variable(0, dtype=tf.int64,   trainable=False, name='global_step')\n",
    "    learning_rate  = tf.placeholder(dtype=tf.float32, name='learning_rate')\n",
    "    gene_opti = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                       beta1=FLAGS.learning_beta1,\n",
    "                                       name='gene_optimizer')\n",
    "    disc_opti = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                       beta1=FLAGS.learning_beta1,\n",
    "                                       name='disc_optimizer')\n",
    "    gene_minimize = gene_opti.minimize(gene_loss, var_list=gene_var_list\n",
    "                                       , name='gene_loss_minimize', global_step=global_step)\n",
    "    disc_minimize = disc_opti.minimize(disc_loss, var_list=disc_var_list\n",
    "                                       , name='disc_loss_minimize', global_step=global_step)\n",
    "    return (global_step, learning_rate, gene_minimize, disc_minimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(global_step, learning_rate, gene_minimize, disc_minimize) = \\\n",
    "    create_optimizers(gene_loss, gene_var_list, disc_loss, disc_var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainData(object):\n",
    "    def __init__(self, dictionary):\n",
    "        self.__dict__.update(dictionary)\n",
    "train_data = TrainData(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _summarize_progress(train_data, feature, label, gene_output, batch, suffix, max_samples=8):\n",
    "    td = train_data\n",
    "    size = [label.shape[1], label.shape[2]]\n",
    "    nearest = tf.image.resize_nearest_neighbor(feature, size)\n",
    "    nearest = tf.maximum(tf.minimum(nearest, 1.0), 0.0)\n",
    "    bicubic = tf.image.resize_bicubic(feature, size)\n",
    "    bicubic = tf.maximum(tf.minimum(bicubic, 1.0), 0.0)\n",
    "    clipped = tf.maximum(tf.minimum(gene_output, 1.0), 0.0)\n",
    "    image   = tf.concat(2, [nearest, bicubic, clipped, label])\n",
    "\n",
    "    image = image[0:max_samples,:,:,:]\n",
    "    image = tf.concat(0, [image[i,:,:,:] for i in range(max_samples)])\n",
    "    image = td.sess.run(image)\n",
    "\n",
    "    _filename = 'batch%06d_%s.png' % (batch, suffix)\n",
    "    filename = os.path.join(FLAGS.train_dir, _filename)\n",
    "    scipy.misc.toimage(image, cmin=0., cmax=1.).save(filename)\n",
    "    print(\"    Saved %s\" % (filename,))\n",
    "    # ADDITIONAL PLOT\n",
    "    currsampleimg = imread(filename)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(currsampleimg)\n",
    "    plt.title(_filename)\n",
    "    plt.show()\n",
    "    \n",
    "def _save_checkpoint(train_data, batch):\n",
    "    td = train_data\n",
    "    oldname = 'checkpoint_old.txt'\n",
    "    newname = 'checkpoint_new.txt'\n",
    "    oldname = os.path.join(FLAGS.checkpoint_dir, oldname)\n",
    "    newname = os.path.join(FLAGS.checkpoint_dir, newname)\n",
    "    # Delete oldest checkpoint\n",
    "    try:\n",
    "        tf.gfile.Remove(oldname)\n",
    "        tf.gfile.Remove(oldname + '.meta')\n",
    "    except:\n",
    "        pass\n",
    "    # Rename old checkpoint\n",
    "    try:\n",
    "        tf.gfile.Rename(newname, oldname)\n",
    "        tf.gfile.Rename(newname + '.meta', oldname + '.meta')\n",
    "    except:\n",
    "        pass\n",
    "    # Generate new checkpoint\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(td.sess, newname)\n",
    "    print(\"    Checkpoint saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_data):\n",
    "    td = train_data\n",
    "    summaries = tf.summary.merge_all()\n",
    "    td.sess.run(tf.global_variables_initializer())\n",
    "    lrval       = FLAGS.learning_rate_start\n",
    "    start_time  = time.time()\n",
    "    done  = False\n",
    "    batch = 0\n",
    "    assert FLAGS.learning_rate_half_life % 10 == 0\n",
    "    # Cache test features and labels (they are small)\n",
    "    test_feature, test_label = td.sess.run([td.test_features, td.test_labels])\n",
    "\n",
    "    while not done:\n",
    "        batch += 1\n",
    "        gene_loss = disc_real_loss = disc_fake_loss = -1.234\n",
    "        feed_dict = {td.learning_rate : lrval}\n",
    "        ops = [td.gene_minimize, td.disc_minimize, td.gene_loss, td.disc_real_loss, td.disc_fake_loss]\n",
    "        _, _, gene_loss, disc_real_loss, disc_fake_loss = td.sess.run(ops, feed_dict=feed_dict)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            # Show we are alive\n",
    "            elapsed = int(time.time() - start_time)/60\n",
    "            print('Progress[%3d%%], ETA[%4dm], Batch [%4d], G_Loss[%3.3f], D_Real_Loss[%3.3f], D_Fake_Loss[%3.3f]' %\n",
    "                  (int(100*elapsed/FLAGS.train_time), FLAGS.train_time - elapsed,\n",
    "                   batch, gene_loss, disc_real_loss, disc_fake_loss))\n",
    "            # Finished?            \n",
    "            current_progress = elapsed / FLAGS.train_time\n",
    "            if current_progress >= 1.0:\n",
    "                done = True\n",
    "            # Update learning rate\n",
    "            if batch % FLAGS.learning_rate_half_life == 0:\n",
    "                lrval *= .5\n",
    "\n",
    "        if batch % FLAGS.summary_period == 0:\n",
    "            # Show progress with test features\n",
    "            feed_dict = {td.gene_minput: test_feature}\n",
    "            gene_output = td.sess.run(td.gene_moutput, feed_dict=feed_dict)\n",
    "            _summarize_progress(td, test_feature, test_label, gene_output, batch, 'out')\n",
    "            \n",
    "        if batch % FLAGS.checkpoint_period == 0:\n",
    "            # Save checkpoint\n",
    "            _save_checkpoint(td, batch)\n",
    "\n",
    "    _save_checkpoint(td, batch)\n",
    "    print('Finished training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress[  0%], ETA[  20m], Batch [  50], G_Loss[0.133], D_Real_Loss[0.720], D_Fake_Loss[0.709]\n",
      "Progress[  0%], ETA[  20m], Batch [ 100], G_Loss[0.119], D_Real_Loss[0.710], D_Fake_Loss[0.633]\n",
      "Progress[  0%], ETA[  20m], Batch [ 150], G_Loss[0.122], D_Real_Loss[0.685], D_Fake_Loss[0.688]\n",
      "Progress[  0%], ETA[  20m], Batch [ 200], G_Loss[0.115], D_Real_Loss[0.695], D_Fake_Loss[0.662]\n",
      "Progress[  0%], ETA[  20m], Batch [ 250], G_Loss[0.118], D_Real_Loss[0.679], D_Fake_Loss[0.667]\n",
      "Progress[  0%], ETA[  20m], Batch [ 300], G_Loss[0.123], D_Real_Loss[0.676], D_Fake_Loss[0.677]\n"
     ]
    }
   ],
   "source": [
    "train_model(train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
