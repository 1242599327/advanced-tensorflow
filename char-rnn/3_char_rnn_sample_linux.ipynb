{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAMPLE CHAR-RNN \n",
    "### LINUX KERNEL SOURCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PACKAGES LOADED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print (\"PACKAGES LOADED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARS AND VOCAB ARE LOADED FROM [data/linux_kernel/chars_vocab.pkl]\n",
      "CORPUS AND DATA ARE LOADED FROM [data/linux_kernel/corpus_data.pkl]\n"
     ]
    }
   ],
   "source": [
    "load_dir    = \"data/linux_kernel\"\n",
    "load_name = os.path.join(load_dir, 'chars_vocab.pkl')\n",
    "with open(load_name, 'rb') as fload:\n",
    "    chars, vocab = cPickle.load(fload)\n",
    "    print (\"CHARS AND VOCAB ARE LOADED FROM [%s]\" % (load_name))\n",
    "load_name = os.path.join(load_dir, 'corpus_data.pkl')\n",
    "with open(load_name, 'rb') as fload:\n",
    "    corpus, data = cPickle.load(fload)\n",
    "    print (\"CORPUS AND DATA ARE LOADED FROM [%s]\" % (load_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD SEQ2SEQ MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK READY\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_length = 1\n",
    "vocab_size = len(vocab)\n",
    "rnn_size   = 128\n",
    "num_layers = 2\n",
    "grad_clip  = 5. \n",
    "\n",
    "# CONSTRUCT RNN MODEL\n",
    "unitcell   = tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n",
    "cell       = tf.nn.rnn_cell.MultiRNNCell([unitcell] * num_layers)\n",
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "targets    = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "istate     = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# WEIGHT\n",
    "with tf.variable_scope('rnnlm') as scope:\n",
    "    # SOFTMAX\n",
    "    try:\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    except ValueError:\n",
    "        scope.reuse_variables()\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    # EMBEDDING MATRIX\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "    # tf.split(split_dim, num_split, value, name='split')\n",
    "    inputs = tf.split(1, seq_length, tf.nn.embedding_lookup(embedding, input_data))\n",
    "    # tf.squeeze(input, axis=None, name=None, squeeze_dims=None)\n",
    "    inputs = [tf.squeeze(_input, [1]) for _input in inputs]\n",
    "\n",
    "# DECODER\n",
    "outputs, last_state = tf.nn.seq2seq.rnn_decoder(inputs, istate, cell\n",
    "                , loop_function=None, scope='rnnlm')\n",
    "output = tf.reshape(tf.concat(1, outputs), [-1, rnn_size])\n",
    "logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "probs  = tf.nn.softmax(logits)\n",
    "\n",
    "# LOSS\n",
    "loss = tf.nn.seq2seq.sequence_loss_by_example([logits], # Input\n",
    "    [tf.reshape(targets, [-1])], # Target\n",
    "    [tf.ones([batch_size * seq_length])], # Weight \n",
    "    vocab_size)\n",
    "\n",
    "# OPTIMIZER\n",
    "cost     = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "final_state = last_state\n",
    "lr       = tf.Variable(0.0, trainable=False)\n",
    "tvars    = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "_optm    = tf.train.AdamOptimizer(lr)\n",
    "optm     = _optm.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "print (\"NETWORK READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESTORE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/linux_kernel/model.ckpt-8000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "ckpt  = tf.train.get_checkpoint_state(load_dir)\n",
    "\n",
    "print (ckpt.model_checkpoint_path)\n",
    "saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "/* {\n",
      "\t\t\t\tchil_load_valuly(tsk, rcu_torture_len,\n",
      "\t\t\t\t !ip))\n",
      "\t\t\t\t\tgoto err = CPUCLOSH:\n",
      "\t\t\t\tshuldure_acquiref(struct kernel_pos, int memor),\n",
      "\t\t\tSCHE_LIME *\n",
      "\t * Su' the CPU agly flock, Inkermacker current kills architecy set, we already' by the ret.\n",
      "\t */\n",
      "}\n",
      "\n",
      "/* This currently count module neker\n",
      " *\t/* A newing rathor load: the list can\n",
      " * the a channel bitwerwist to this\n",
      " * not.  Prost inc it any/clones;\n",
      "\tint ret = addr;\n",
      "\n",
      "\treturn q;\n",
      "}\n",
      "\n",
      "static int\n",
      " * dirate(skb);\n",
      "\tunlock_lock(pusix_load_file);\n",
      "\tspin_and();\n",
      "#endif\n",
      "\n",
      "/*\n",
      " * We uses it not->class.h>\n",
      "\n",
      " * Locks us gon't end. The\n",
      " * S@remory initialize?\n",
      " */\n",
      "static struct number_get_print_kset(char *name)\n",
      "{\n",
      "\ttsk.\\n\");\n",
      "EXPORT_SYMBOL(boold_to_rootlock);\n",
      "\t/* str->it_pending == table[] ==;\n",
      "\ttop_subbuf_size + allow_userspace *m2 = pid;\n",
      "\tstruct file *file.  CONFIG_SMP ||\n",
      "\t    !atomic_of(str)))\n",
      "\t\treturn -EREME ;\n",
      "\tspin_unlock_hash flags & ~PAGE (CPUCL)\n",
      "\t\t\t\tret = audit_chaid;\n",
      "\t\t\t}\n",
      "\t\t}\n",
      "\t\tblocked_load_vector(&up->->wm2)\n",
      "\t\t\tif (!this, u)->list_signal->flags) {\n",
      "\t\tstruct task_struct *printk_path * NSEC_PX_MEMABLASH:\n",
      "\t\tu64\n",
      "\t\t*printk(KERN_ERR do Copyriverytich\n",
      "Sif neq_dequeued(int audit_task_struct)\n",
      "{\n",
      "\tstruct kimate_interval = cputime_ots;\n",
      "\tprintk_lock();\n",
      "#endif\n",
      "\n",
      "\tdescrimner_class++;\n",
      "\n",
      "\tfiler(event);\n",
      "\n",
      "\tif (!delt);\n",
      "\treturn 0;\n",
      "}\n",
      "\n",
      "/**\n",
      " * accoundigialized.  Inder approgram.\n",
      " */\n",
      "asmlinkage conter fierates\n",
      " *\tthe has flocked readdate the softiction the in-20->cfs_rq to bim/if 09-1) and\n",
      " * @reamay: anot to get a handler7\n",
      " * it when we achation\n",
      " */\n",
      "void ctl_table return resource;\n",
      "\n",
      "\tretval = 0;\n",
      "\tif (error) {\n",
      "\t\tstruct list_head power;\n",
      "\t\tstruct task_struct freen = get_relesaged > 20000x, IN_IRQ_ROUTE,\n",
      "\t\t\t\t.data.neffuf: + strtult_chunk reprogram].sh_cpu,\n",
      "\t\t\t\tBITK_INIT_CONSDEF;\n",
      "\t\tkfreem(struct list_entry);\n",
      "\t\t} else\n",
      "\t\t\tset_set_sched();\n",
      "\tp->slote = try_mm(\"members\", fadded, fs))\n",
      "\t\t\tgoto out_data;\n",
      "\t\tcpu[05:\t\t\t\t\t\t\t\tint nr_set;\n",
      "\t\t\t\taudit_log_futex(table->cpu_bits.cpu = &panic_chain_key_program(event);\n",
      "\t\t\treturn -ERESTOPPED;\n",
      "\t\t} else\n",
      "\t\t\ti+=#. { }\n",
      "#endif\n",
      "\n",
      "\tif (cpus_rq_in\n"
     ]
    }
   ],
   "source": [
    "# Sampling function\n",
    "def weighted_pick(weights):\n",
    "    t = np.cumsum(weights)\n",
    "    s = np.sum(weights)\n",
    "    return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "# Sample using RNN and prime characters\n",
    "prime = \"/* \"\n",
    "state = sess.run(cell.zero_state(1, tf.float32))\n",
    "for char in prime[:-1]:\n",
    "    x = np.zeros((1, 1))\n",
    "    x[0, 0] = vocab[char]\n",
    "    state = sess.run(final_state, feed_dict={input_data: x, istate:state})\n",
    "\n",
    "# Sample 'num' characters\n",
    "ret  = prime\n",
    "char = prime[-1] # <= This goes IN! \n",
    "num  = 2000\n",
    "for n in range(num):\n",
    "    x = np.zeros((1, 1))\n",
    "    x[0, 0] = vocab[char]\n",
    "    [probsval, state] = sess.run([probs, final_state]\n",
    "        , feed_dict={input_data: x, istate:state})\n",
    "    p      = probsval[0] \n",
    "    sample = weighted_pick(p)\n",
    "    # sample = np.argmax(p)\n",
    "    pred   = chars[sample]\n",
    "    ret    = ret + pred\n",
    "    char   = pred\n",
    "    \n",
    "print (\"_\"*100)\n",
    "print (ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
