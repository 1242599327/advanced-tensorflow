{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN CHAR-RNN \n",
    "### LINUX KERNEL SOURCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PACKAGES LOADED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print (\"PACKAGES LOADED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARS AND VOCAB ARE LOADED FROM [data/linux_kernel/chars_vocab.pkl]\n",
      "CORPUS AND DATA ARE LOADED FROM [data/linux_kernel/corpus_data.pkl]\n"
     ]
    }
   ],
   "source": [
    "load_dir    = \"data/linux_kernel\"\n",
    "load_name = os.path.join(load_dir, 'chars_vocab.pkl')\n",
    "with open(load_name, 'rb') as fload:\n",
    "    chars, vocab = cPickle.load(fload)\n",
    "    print (\"CHARS AND VOCAB ARE LOADED FROM [%s]\" % (load_name))\n",
    "load_name = os.path.join(load_dir, 'corpus_data.pkl')\n",
    "with open(load_name, 'rb') as fload:\n",
    "    corpus, data = cPickle.load(fload)\n",
    "    print (\"CORPUS AND DATA ARE LOADED FROM [%s]\" % (load_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE XDATA AND YDATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_BATCHES IS 170\n",
      "XDATA IS [36 22  7 ..., 11 25  3] / TYPE IS <type 'numpy.ndarray'> / SHAPE IS (1700000,)\n",
      "YDATA IS [22  7  0 ..., 25  3 36] / TYPE IS <type 'numpy.ndarray'> / SHAPE IS (1700000,)\n"
     ]
    }
   ],
   "source": [
    "batch_size  = 50\n",
    "seq_length  = 200\n",
    "num_batches = int(corpus.size / (batch_size * seq_length))\n",
    "print (\"NUM_BATCHES IS %s\" % (num_batches))\n",
    "\n",
    "corpus_reduced = corpus[:(num_batches*batch_size*seq_length)]\n",
    "xdata = corpus_reduced\n",
    "ydata = np.copy(xdata)\n",
    "ydata[:-1] = xdata[1:]\n",
    "ydata[-1]  = xdata[0]\n",
    "print ('XDATA IS %s / TYPE IS %s / SHAPE IS %s' % (xdata, type(xdata), xdata.shape))\n",
    "print ('YDATA IS %s / TYPE IS %s / SHAPE IS %s' % (ydata, type(ydata), ydata.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE: 50 / NUM_BATCHES: 170\n",
      "TYPE OF 'XBATCHES' IS <type 'list'> AND LENGTH IS 170\n",
      "TYPE OF 'YBATCHES' IS <type 'list'> AND LENGTH IS 170\n",
      "TYPE OF EACH BATCH IS <type 'numpy.ndarray'> AND SHAPE IS (50, 200)\n"
     ]
    }
   ],
   "source": [
    "xbatches = np.split(xdata.reshape(batch_size, -1), num_batches, 1)\n",
    "ybatches = np.split(ydata.reshape(batch_size, -1), num_batches, 1)\n",
    "\n",
    "print (\"BATCH_SIZE: %d / NUM_BATCHES: %d\" % (batch_size, num_batches))\n",
    "print (\"TYPE OF 'XBATCHES' IS %s AND LENGTH IS %d\" \n",
    "    % (type(xbatches), len(xbatches)))\n",
    "print (\"TYPE OF 'YBATCHES' IS %s AND LENGTH IS %d\"\n",
    "    % (type(ybatches), len(ybatches)))\n",
    "print (\"TYPE OF EACH BATCH IS %s AND SHAPE IS %s\" \n",
    "    % (type(xbatches[0]), (xbatches[0]).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XBATCHES & YBATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========XBATCHES===========\n",
      "[[36 22  7 ...,  1 15  1]\n",
      " [ 6 19 14 ...,  7  6 32]\n",
      " [12 24 13 ...,  2 10 13]\n",
      " ..., \n",
      " [21 62 12 ..., 10  5 14]\n",
      " [17  3  1 ..., 21 54 20]\n",
      " [14 21 12 ...,  7  6 44]]\n",
      "===========YBATCHES===========\n",
      "[[22  7  0 ..., 15  1  8]\n",
      " [19 14  2 ...,  6 32 18]\n",
      " [24 13 11 ..., 10 13 11]\n",
      " ..., \n",
      " [62 12  9 ...,  5 14 27]\n",
      " [ 3  1  1 ..., 54 20  7]\n",
      " [21 12 16 ...,  6 44  7]]\n"
     ]
    }
   ],
   "source": [
    "print (\"===========XBATCHES===========\")\n",
    "print (xbatches[0])\n",
    "print (\"===========YBATCHES===========\")\n",
    "print (ybatches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD SEQ2SEQ MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK READY\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "rnn_size   = 128\n",
    "num_layers = 2\n",
    "grad_clip  = 5. \n",
    "\n",
    "# CONSTRUCT RNN MODEL\n",
    "unitcell   = tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n",
    "cell       = tf.nn.rnn_cell.MultiRNNCell([unitcell] * num_layers)\n",
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "targets    = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "istate     = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# WEIGHT\n",
    "with tf.variable_scope('rnnlm') as scope:\n",
    "    # SOFTMAX\n",
    "    try:\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    except ValueError:\n",
    "        scope.reuse_variables()\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    # EMBEDDING MATRIX\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "    # tf.split(split_dim, num_split, value, name='split')\n",
    "    inputs = tf.split(1, seq_length, tf.nn.embedding_lookup(embedding, input_data))\n",
    "    # tf.squeeze(input, axis=None, name=None, squeeze_dims=None)\n",
    "    inputs = [tf.squeeze(_input, [1]) for _input in inputs]\n",
    "\n",
    "# DECODER\n",
    "outputs, last_state = tf.nn.seq2seq.rnn_decoder(inputs, istate, cell\n",
    "                , loop_function=None, scope='rnnlm')\n",
    "output = tf.reshape(tf.concat(1, outputs), [-1, rnn_size])\n",
    "logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "probs  = tf.nn.softmax(logits)\n",
    "\n",
    "# LOSS\n",
    "loss = tf.nn.seq2seq.sequence_loss_by_example([logits], # Input\n",
    "    [tf.reshape(targets, [-1])], # Target\n",
    "    [tf.ones([batch_size * seq_length])], # Weight \n",
    "    vocab_size)\n",
    "\n",
    "# OPTIMIZER\n",
    "cost     = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "final_state = last_state\n",
    "lr       = tf.Variable(0.0, trainable=False)\n",
    "tvars    = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "_optm    = tf.train.AdamOptimizer(lr)\n",
    "optm     = _optm.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "print (\"NETWORK READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/8500] cost: 5.0688 / Each batch learning took 1.9799 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[100/8500] cost: 3.0738 / Each batch learning took 0.0790 sec\n",
      "[200/8500] cost: 2.6460 / Each batch learning took 0.0848 sec\n",
      "[300/8500] cost: 2.3816 / Each batch learning took 0.0785 sec\n",
      "[400/8500] cost: 2.1956 / Each batch learning took 0.0791 sec\n",
      "[500/8500] cost: 2.0221 / Each batch learning took 0.0795 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[600/8500] cost: 1.9355 / Each batch learning took 0.0782 sec\n",
      "[700/8500] cost: 1.8261 / Each batch learning took 0.0781 sec\n",
      "[800/8500] cost: 1.8399 / Each batch learning took 0.0792 sec\n",
      "[900/8500] cost: 1.7405 / Each batch learning took 0.0778 sec\n",
      "[1000/8500] cost: 1.6442 / Each batch learning took 0.0831 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[1100/8500] cost: 1.6320 / Each batch learning took 0.0778 sec\n",
      "[1200/8500] cost: 1.5566 / Each batch learning took 0.0778 sec\n",
      "[1300/8500] cost: 1.6454 / Each batch learning took 0.0818 sec\n",
      "[1400/8500] cost: 1.5218 / Each batch learning took 0.0795 sec\n",
      "[1500/8500] cost: 1.5232 / Each batch learning took 0.0787 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[1600/8500] cost: 1.4934 / Each batch learning took 0.0804 sec\n",
      "[1700/8500] cost: 1.4255 / Each batch learning took 0.0824 sec\n",
      "[1800/8500] cost: 1.4111 / Each batch learning took 0.0779 sec\n",
      "[1900/8500] cost: 1.5481 / Each batch learning took 0.0779 sec\n",
      "[2000/8500] cost: 1.4412 / Each batch learning took 0.0778 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[2100/8500] cost: 1.4545 / Each batch learning took 0.0775 sec\n",
      "[2200/8500] cost: 1.4415 / Each batch learning took 0.0792 sec\n",
      "[2300/8500] cost: 1.3933 / Each batch learning took 0.0788 sec\n",
      "[2400/8500] cost: 1.3559 / Each batch learning took 0.0777 sec\n",
      "[2500/8500] cost: 1.3534 / Each batch learning took 0.0836 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[2600/8500] cost: 1.4030 / Each batch learning took 0.0827 sec\n",
      "[2700/8500] cost: 1.3730 / Each batch learning took 0.0776 sec\n",
      "[2800/8500] cost: 1.3549 / Each batch learning took 0.0778 sec\n",
      "[2900/8500] cost: 1.3603 / Each batch learning took 0.0780 sec\n",
      "[3000/8500] cost: 1.3255 / Each batch learning took 0.0778 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[3100/8500] cost: 1.3550 / Each batch learning took 0.0774 sec\n",
      "[3200/8500] cost: 1.3559 / Each batch learning took 0.0796 sec\n",
      "[3300/8500] cost: 1.3038 / Each batch learning took 0.0832 sec\n",
      "[3400/8500] cost: 1.2913 / Each batch learning took 0.0782 sec\n",
      "[3500/8500] cost: 1.3173 / Each batch learning took 0.0808 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[3600/8500] cost: 1.2930 / Each batch learning took 0.0791 sec\n",
      "[3700/8500] cost: 1.4089 / Each batch learning took 0.0775 sec\n",
      "[3800/8500] cost: 1.3114 / Each batch learning took 0.0777 sec\n",
      "[3900/8500] cost: 1.2654 / Each batch learning took 0.0778 sec\n",
      "[4000/8500] cost: 1.2696 / Each batch learning took 0.0781 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[4100/8500] cost: 1.2911 / Each batch learning took 0.0777 sec\n",
      "[4200/8500] cost: 1.2600 / Each batch learning took 0.0786 sec\n",
      "[4300/8500] cost: 1.2491 / Each batch learning took 0.0795 sec\n",
      "[4400/8500] cost: 1.2444 / Each batch learning took 0.0811 sec\n",
      "[4500/8500] cost: 1.2735 / Each batch learning took 0.0829 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[4600/8500] cost: 1.2905 / Each batch learning took 0.0795 sec\n",
      "[4700/8500] cost: 1.2652 / Each batch learning took 0.0779 sec\n",
      "[4800/8500] cost: 1.2372 / Each batch learning took 0.0778 sec\n",
      "[4900/8500] cost: 1.2038 / Each batch learning took 0.0791 sec\n",
      "[5000/8500] cost: 1.2612 / Each batch learning took 0.0777 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[5100/8500] cost: 1.1928 / Each batch learning took 0.0824 sec\n",
      "[5200/8500] cost: 1.2235 / Each batch learning took 0.0780 sec\n",
      "[5300/8500] cost: 1.2442 / Each batch learning took 0.0784 sec\n",
      "[5400/8500] cost: 1.2575 / Each batch learning took 0.0780 sec\n",
      "[5500/8500] cost: 1.1807 / Each batch learning took 0.0809 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[5600/8500] cost: 1.2449 / Each batch learning took 0.0780 sec\n",
      "[5700/8500] cost: 1.2663 / Each batch learning took 0.0779 sec\n",
      "[5800/8500] cost: 1.2071 / Each batch learning took 0.0784 sec\n",
      "[5900/8500] cost: 1.2291 / Each batch learning took 0.0800 sec\n",
      "[6000/8500] cost: 1.1494 / Each batch learning took 0.0780 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[6100/8500] cost: 1.1474 / Each batch learning took 0.0780 sec\n",
      "[6200/8500] cost: 1.2154 / Each batch learning took 0.0782 sec\n",
      "[6300/8500] cost: 1.2653 / Each batch learning took 0.0779 sec\n",
      "[6400/8500] cost: 1.2140 / Each batch learning took 0.0789 sec\n",
      "[6500/8500] cost: 1.1868 / Each batch learning took 0.0781 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[6600/8500] cost: 1.2001 / Each batch learning took 0.0790 sec\n",
      "[6700/8500] cost: 1.1934 / Each batch learning took 0.0781 sec\n",
      "[6800/8500] cost: 1.1925 / Each batch learning took 0.0784 sec\n",
      "[6900/8500] cost: 1.2226 / Each batch learning took 0.0782 sec\n",
      "[7000/8500] cost: 1.1664 / Each batch learning took 0.0780 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[7100/8500] cost: 1.1972 / Each batch learning took 0.0783 sec\n",
      "[7200/8500] cost: 1.1507 / Each batch learning took 0.0787 sec\n",
      "[7300/8500] cost: 1.1817 / Each batch learning took 0.0859 sec\n",
      "[7400/8500] cost: 1.1762 / Each batch learning took 0.0831 sec\n",
      "[7500/8500] cost: 1.1299 / Each batch learning took 0.0796 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[7600/8500] cost: 1.1982 / Each batch learning took 0.0783 sec\n",
      "[7700/8500] cost: 1.2421 / Each batch learning took 0.0777 sec\n",
      "[7800/8500] cost: 1.1783 / Each batch learning took 0.0776 sec\n",
      "[7900/8500] cost: 1.1570 / Each batch learning took 0.0781 sec\n",
      "[8000/8500] cost: 1.1454 / Each batch learning took 0.0784 sec\n",
      "model saved to 'data/linux_kernel/model.ckpt'\n",
      "[8100/8500] cost: 1.2125 / Each batch learning took 0.0781 sec\n",
      "[8200/8500] cost: 1.1646 / Each batch learning took 0.0783 sec\n",
      "[8300/8500] cost: 1.1479 / Each batch learning took 0.0781 sec\n",
      "[8400/8500] cost: 1.2539 / Each batch learning took 0.0827 sec\n"
     ]
    }
   ],
   "source": [
    "save_dir      = \"data/linux_kernel\"\n",
    "num_epochs    = 50\n",
    "save_every    = 500\n",
    "learning_rate = 0.002\n",
    "decay_rate    = 0.97\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "summary_writer = tf.summary.FileWriter(save_dir, graph=sess.graph)\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "init_time = time.time()\n",
    "for epoch in range(num_epochs): # FOR EACH EPOCH\n",
    "    sess.run(tf.assign(lr, learning_rate * (decay_rate ** epoch)))\n",
    "    state     = sess.run(istate)\n",
    "    batchidx  = 0\n",
    "    randbatchidx = np.random.permutation(num_batches)\n",
    "    for iteration in range(num_batches): # FOR EACH ITERATION\n",
    "        batchidx     = batchidx + 1\n",
    "        xbatch       = xbatches[randbatchidx[batchidx]]\n",
    "        ybatch       = ybatches[randbatchidx[batchidx]]\n",
    "        \n",
    "        start_time   = time.time()\n",
    "        train_loss, state, _ = sess.run([cost, final_state, optm]\n",
    "            , feed_dict={input_data: xbatch, targets: ybatch, istate: state}) \n",
    "        total_iter = epoch*num_batches + iteration\n",
    "        end_time   = time.time();\n",
    "        duration   = end_time - start_time\n",
    "        \n",
    "        if total_iter % 100 == 0:\n",
    "            print (\"[%d/%d] cost: %.4f / Each batch learning took %.4f sec\" \n",
    "                   % (total_iter, num_epochs*num_batches, train_loss, duration))\n",
    "        if total_iter % save_every == 0: \n",
    "            ckpt_path = os.path.join(save_dir, 'model.ckpt')\n",
    "            saver.save(sess, ckpt_path, global_step = total_iter)\n",
    "            print(\"model saved to '%s'\" % (ckpt_path)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
